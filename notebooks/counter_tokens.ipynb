{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3e8e7e-3ae7-45d9-aabe-d7d8fe64db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6093e17-0514-49e7-9e8b-e5d83f252d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def contar_tokens_correctamente(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Lee un archivo JSONL y cuenta los tokens usando el método oficial\n",
    "    `encode_chat_completion` de la biblioteca mistral-common.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo (str): La ruta al archivo .jsonl.\n",
    "    \"\"\"\n",
    "    # Define el nombre del modelo que se usará tanto para cargar el tokenizador\n",
    "    # como para crear la solicitud de chat.\n",
    "    model_name = \"mistral-small-latest\" # o el modelo que uses\n",
    "\n",
    "    try:\n",
    "        tokenizer = MistralTokenizer.from_model(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el tokenizador: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Procesando archivo: {ruta_archivo} con el tokenizador de {model_name}\\n\")\n",
    "    docs = []\n",
    "    query = []\n",
    "    answer = []\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "        for i, linea in enumerate(f):\n",
    "            try:\n",
    "                dato = json.loads(linea.strip())\n",
    "\n",
    "                # Extraer los textos de los campos requeridos\n",
    "                texto_documento = dato.get(\"generated_document\", {})[0]\n",
    "                texto_query = dato.get(\"generated_query\", {})[0]\n",
    "                texto_respuesta = dato.get(\"generated_answer\", {})[0]\n",
    "\n",
    "                # --- LÓGICA DE TOKENIZACIÓN CORRECTA ---\n",
    "                # Para cada texto, creamos una solicitud y la tokenizamos.\n",
    "\n",
    "                # 1. Tokenizar el documento generado\n",
    "                req_doc = ChatCompletionRequest(model=model_name, messages=[UserMessage(content=texto_documento)])\n",
    "                tokens_documento = len(tokenizer.encode_chat_completion(req_doc).tokens)\n",
    "                docs.append(tokens_documento)\n",
    "                # 2. Tokenizar la query generada\n",
    "                req_query = ChatCompletionRequest(model=model_name, messages=[UserMessage(content=texto_query)])\n",
    "                tokens_query = len(tokenizer.encode_chat_completion(req_query).tokens)\n",
    "                query.append(tokens_query)\n",
    "                # 3. Tokenizar la respuesta generada\n",
    "                req_resp = ChatCompletionRequest(model=model_name, messages=[UserMessage(content=texto_respuesta)])\n",
    "                tokens_respuesta = len(tokenizer.encode_chat_completion(req_resp).tokens)\n",
    "                answer.append(tokens_respuesta)\n",
    "                # # Imprimir el resultado para la línea actual\n",
    "                # print(f\"Línea {i + 1}:\")\n",
    "                # print(f\"  - Documento: {tokens_documento} tokens\")\n",
    "                # print(f\"  - Query generada: {tokens_query} tokens\")\n",
    "                # print(f\"  - Respuesta generada: {tokens_respuesta} tokens\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Advertencia: La línea {i + 1} no es un JSON válido y será omitida.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando la línea {i + 1}: {e}\")\n",
    "\n",
    "    print(\"\\n--- Proceso completado ---\")\n",
    "    return {\"gen_docs\":docs, \"gen_query\":query, \"gen_answer\":answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7db43ba0-3b5a-4c26-b287-e863e02ece2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: ../doc_gen/fire/doc5/bge_large/generated_documents_nfcorpus.jsonl con el tokenizador de mistral-small-latest\n",
      "\n",
      "\n",
      "--- Proceso completado ---\n"
     ]
    }
   ],
   "source": [
    "# --- EJECUCIÓN DEL SCRIPT ---\n",
    "# Reemplaza 'tu_archivo.jsonl' con el nombre de tu archivo\n",
    "nombre_del_archivo_jsonl = '../doc_gen/fire/doc5/bge_large/generated_documents_nfcorpus.jsonl'\n",
    "result_fire = contar_tokens_correctamente(nombre_del_archivo_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fe8a25f-2ec9-43a7-97d4-9173c8d0fd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de tokens es: 357.70588235294116\n"
     ]
    }
   ],
   "source": [
    "promedio = sum(result_fire['gen_docs']) / len(result_fire['gen_docs'])\n",
    "print(f\"El promedio de tokens es: {promedio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aa1726f-5878-4130-81f0-23388e66021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de tokens es: 57.417956656346746\n"
     ]
    }
   ],
   "source": [
    "promedio = sum(result_fire['gen_query']) / len(result_fire['gen_query'])\n",
    "print(f\"El promedio de tokens es: {promedio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c71d356d-d4ff-4ac2-bb80-37ed2db3fde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de tokens es: 177.03405572755418\n"
     ]
    }
   ],
   "source": [
    "promedio = sum(result_fire['gen_answer']) / len(result_fire['gen_answer'])\n",
    "print(f\"El promedio de tokens es: {promedio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0689c9-7fe8-4ab5-97a3-ab172c9f5e77",
   "metadata": {},
   "source": [
    "## hyde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e42de1db-3c31-46e6-b534-45118776fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def contar_tokens_hyde_correctamente(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Lee un archivo JSONL y cuenta los tokens usando el método oficial\n",
    "    `encode_chat_completion` de la biblioteca mistral-common.\n",
    "\n",
    "    Args:\n",
    "        ruta_archivo (str): La ruta al archivo .jsonl.\n",
    "    \"\"\"\n",
    "    # Define el nombre del modelo que se usará tanto para cargar el tokenizador\n",
    "    # como para crear la solicitud de chat.\n",
    "    model_name = \"mistral-small-latest\" # o el modelo que uses\n",
    "\n",
    "    try:\n",
    "        tokenizer = MistralTokenizer.from_model(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el tokenizador: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Procesando archivo: {ruta_archivo} con el tokenizador de {model_name}\\n\")\n",
    "\n",
    "    docs = []\n",
    "    with open(ruta_archivo, 'r', encoding='utf-8') as f:\n",
    "        for i, linea in enumerate(f):\n",
    "            try:\n",
    "                dato = json.loads(linea.strip())\n",
    "                documents = dato.get(\"generated_documents\", [])\n",
    "                len_docs=0\n",
    "                for j, doc in enumerate(documents):\n",
    "\n",
    "                    # 1. Tokenizar el documento generado\n",
    "                    req_doc = ChatCompletionRequest(model=model_name, messages=[UserMessage(content=doc)])\n",
    "                    tokens_documento = len(tokenizer.encode_chat_completion(req_doc).tokens)\n",
    "                    len_docs+=tokens_documento\n",
    "                docs.append(len_docs)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Advertencia: La línea {i + 1} no es un JSON válido y será omitida.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando la línea {i + 1}: {e}\")\n",
    "\n",
    "    print(\"\\n--- Proceso completado ---\")\n",
    "    return {\"gen_docs\":docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3234ea2-128b-485b-a23f-4ec4a3798c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: ../doc_gen/hyde/generated_documents_nfcorpus.jsonl con el tokenizador de mistral-small-latest\n",
      "\n",
      "\n",
      "--- Proceso completado ---\n"
     ]
    }
   ],
   "source": [
    "nombre_del_archivo_jsonl_hyde = '../doc_gen/hyde/generated_documents_nfcorpus.jsonl'\n",
    "result_hyde = contar_tokens_hyde_correctamente(nombre_del_archivo_jsonl_hyde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ebff402-b5df-4a52-aec8-ba1622101606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de tokens es: 2387.2260061919505\n"
     ]
    }
   ],
   "source": [
    "promedio = sum(result_hyde['gen_docs']) / len(result_hyde['gen_docs'])\n",
    "print(f\"El promedio de tokens es: {promedio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86c3e779-2753-40a8-976d-3107d1c049cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: ../doc_gen/query2doc/generated_documents_nfcorpus.jsonl con el tokenizador de mistral-small-latest\n",
      "\n",
      "\n",
      "--- Proceso completado ---\n"
     ]
    }
   ],
   "source": [
    "nombre_del_archivo_jsonl_q2docs = '../doc_gen/query2doc/generated_documents_nfcorpus.jsonl'\n",
    "result_q2d = contar_tokens_hyde_correctamente(nombre_del_archivo_jsonl_q2docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4ecb25d-6219-4a95-ae08-b9e2132790ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El promedio de tokens es: 444.05882352941177\n"
     ]
    }
   ],
   "source": [
    "promedio = sum(result_q2d['gen_docs']) / len(result_q2d['gen_docs'])\n",
    "print(f\"El promedio de tokens es: {promedio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1b0fc-42cc-43f8-a05d-8d47ce44a702",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

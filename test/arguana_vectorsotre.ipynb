{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba95a0b1-e24e-4491-9070-656100dd5dc5",
   "metadata": {},
   "source": [
    "## Arguana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce897e06-3180-46db-ac25-84f074b05f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local1/ronaldinho/enviroments/env_kdir/lib/python3.10/site-packages/beir/util.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11771dd0-94a0-45bf-9088-c94b858fb612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beir_dataset(dataset_name):\n",
    "    dataset = dataset_name\n",
    "    url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "    data_path = util.download_and_unzip(url, \"datasets\")\n",
    "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    return corpus, queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1568afa4-f668-4d8c-af51-ca1336d4364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_corpus_arguana(corpus):\n",
    "    sentences = []\n",
    "    payloads = []\n",
    "    all_doc_items = list(corpus.items())\n",
    "    for doc_id, contenido in tqdm(all_doc_items, desc=\"Procesando corpus\"):\n",
    "        payload = {\n",
    "            'doc_id': doc_id,\n",
    "            'title': contenido['title'],\n",
    "            'text': contenido['text']\n",
    "        }\n",
    "        payloads.append(payload)\n",
    "        sentences.append(contenido['text'])\n",
    "    return sentences, payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9215078-1314-4845-9c66-330d6f389a56",
   "metadata": {},
   "source": [
    "## Qdrant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc38a8c-00d7-47ad-a8ea-274feac6c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, SparseVectorParams, SparseVector\n",
    "from tqdm import tqdm\n",
    "#client = QdrantClient(url=\"http://localhost:6333\")\n",
    "#client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb657824-94ef-4385-a66d-4888e07172d0",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204c8b4-e250-4834-9212-cc561d17b1f1",
   "metadata": {},
   "source": [
    "### Contriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff1931e-0ef1-41c6-8cdf-0ed348b7c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastembed import SparseTextEmbedding, SparseEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3b3609-9cab-46af-927f-095324cb1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_contriever():\n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    embedding_dimension = model.config.hidden_size\n",
    "    return tokenizer, model, embedding_dimension\n",
    "\n",
    "def get_contriever_embeddings(tokenizer, model, sentences):\n",
    "    def mean_pooling(token_embeddings, mask):\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        return sentence_embeddings\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a337cf4-22f1-4d19-a16f-b194e687133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_dpr():\n",
    "    tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "    model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "    embedding_dimension = model.config.hidden_size\n",
    "    return tokenizer, model, embedding_dimension\n",
    "\n",
    "def load_context_dpr():\n",
    "    tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    embedding_dimension = model.config.hidden_size\n",
    "    return tokenizer, model, embedding_dimension\n",
    "\n",
    "def get_dpr_embeddings(tokenizer, model, sentences):\n",
    "    max_model_input_length = model.config.max_position_embeddings\n",
    "    if max_model_input_length is None:\n",
    "        max_model_input_length = 512 \n",
    "    inputs = tokenizer(\n",
    "        sentences, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=max_model_input_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    embeddings = model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdd5c84-27f7-4e36-ac3d-e997bcf67c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bge_large():\n",
    "    model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "    embedding_dimension = model.get_sentence_embedding_dimension()\n",
    "    return model, embedding_dimension\n",
    "\n",
    "def get_bge_large_embeddings(model, sentences):\n",
    "    embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53adede4-b85a-4d12-8e7e-d2732a746e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bm25():\n",
    "    model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n",
    "    return model\n",
    "    \n",
    "def load_splade():\n",
    "    model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")\n",
    "    return model\n",
    "\n",
    "def get_sparse_embeddings(model, sentences, batch_size=6):\n",
    "    sparse_embeddings_list: list[SparseEmbedding] = list(\n",
    "        model.embed(sentences, batch_size=batch_size)\n",
    "    ) \n",
    "    return sparse_embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61e66fde-b0b8-43e6-90e3-35f586e12bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    print('Loading contriever...')\n",
    "    contriever = load_contriever()\n",
    "    print('Loading question dpr...')\n",
    "    question_dpr = load_question_dpr()\n",
    "    print('Loading context dpr...')\n",
    "    context_dpr = load_context_dpr()\n",
    "    print('Loading bge large...')\n",
    "    bge_large = load_bge_large()\n",
    "    print('Loading bm25...')\n",
    "    bm25 = load_bm25()\n",
    "    print('Loading splade...')\n",
    "    splade = load_splade()\n",
    "    models = {\n",
    "        'contriever': contriever,\n",
    "        'question_dpr': question_dpr,\n",
    "        'context_dpr': context_dpr,\n",
    "        'bge_large': bge_large,\n",
    "        'bm25': bm25,\n",
    "        'splade': splade\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b76ba6f-6adb-4c2f-95a4-9194b678b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ee373b-39d9-4917-bf54-1a54ff961643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus, queries, qrels = get_beir_dataset(\"arguana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1023fba9-de95-4dd7-a5b7-a4b6b7ece8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences, payloads = get_sentences_corpus_arguana(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ac7968-f8a8-453b-b17d-8bb268822c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_embeddings(models, sentences):\n",
    "    contriever_embeddings = get_contriever_embeddings(models['contriever'][0],models['contriever'][1], sentences)\n",
    "    dpr_embeddings = get_dpr_embeddings(models['context_dpr'][0],models['context_dpr'][1], sentences)\n",
    "    bge_embeddings = get_bge_large_embeddings(models['bge_large'][0], sentences)\n",
    "    bm25_embeddings = get_sparse_embeddings(models['bm25'], sentences)\n",
    "    splade_embeddings = get_sparse_embeddings(models['splade'], sentences)\n",
    "    return [contriever_embeddings, dpr_embeddings, bge_embeddings, bm25_embeddings, splade_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5996fd8-d5c3-450b-867d-a99e70e6450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = get_docs_embeddings(models, sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6702a546-840c-444c-af35-2c8df4568bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(client, collection_name,  DIM_CONTRIEVER, DIM_DPR, DIM_BGE_L): \n",
    "    client.create_collection(\n",
    "        collection_name= collection_name,\n",
    "        vectors_config={\n",
    "            \"contriever\": VectorParams(size=DIM_CONTRIEVER, distance=Distance.COSINE),\n",
    "            \"dpr\": VectorParams(size=DIM_DPR, distance=Distance.COSINE),\n",
    "            \"bge_large\": VectorParams(size=DIM_BGE_L, distance=Distance.COSINE),\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"sparse_bm25\": SparseVectorParams(),\n",
    "            \"sparse_splade\": SparseVectorParams(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9b1aa01-194b-477e-bc7d-2345f0e135a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(all_embeddings,payloads):\n",
    "    points = []\n",
    "    for i, payload in tqdm(enumerate(payloads), desc= \"creando points\"):\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=int(i), \n",
    "                payload=payload,\n",
    "                vector={\n",
    "                    \"contriever\":all_embeddings[0][i],\n",
    "                    \"dpr\": all_embeddings[1][i],\n",
    "                    \"bge_large\": all_embeddings[2][i],\n",
    "                    \"sparse_bm25\": SparseVector(\n",
    "                        indices=all_embeddings[3][i].indices,\n",
    "                        values=all_embeddings[3][i].values\n",
    "                    ),\n",
    "                    \"sparse_splade\": SparseVector(\n",
    "                        indices=all_embeddings[4][i].indices,\n",
    "                        values=all_embeddings[4][i].values\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1c0cad3-9244-4982-9fd9-1dea282bef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pon = get_points(embeddings,payloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "021af165-c3cb-4702-8389-ace238b3e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_documents(collection_name, points, batch_size=100):\n",
    "    total_batches = math.ceil(len(points) / batch_size)\n",
    "    for i in tqdm(range(total_batches), desc=\"Insertando documentos en lotes\"):\n",
    "        batch = points[i * batch_size : (i + 1) * batch_size]\n",
    "        client.upsert(collection_name=\"scifact\", points=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25bb6661-a88e-43fa-9a57-987c9d24b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(dataset_name):\n",
    "    client = QdrantClient(url=\"http://localhost:6333\")\n",
    "    models = load_models()\n",
    "    DIM_CONTRIEVER = models['contriever'][2]\n",
    "    DIM_DPR = models['context_dpr'][2]\n",
    "    DIM_BGE_L = models['bge_large'][1]\n",
    "    collection_name= f\"kdir_{dataset_name}\"\n",
    "    create_collection(client, collection_name,  DIM_CONTRIEVER, DIM_DPR, DIM_BGE_L)\n",
    "    corpus, queries, qrels = get_beir_dataset(dataset_name)\n",
    "    sentences, payloads = get_sentences_corpus_arguana(corpus)\n",
    "    all_embeddings = get_docs_embeddings(models, sentences)\n",
    "    points = get_points(embeddings,payloads)\n",
    "    insert_documents(collection_name, points, batch_size=100)\n",
    "    print(\"Coleccion creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "472c4548-64b6-4756-b467-98b7436619f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "client.get_collections()\n",
    "client.delete_collection(collection_name=\"kdir_arguana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9eb81-c2ad-4718-98d7-4ae964924cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading contriever...\n",
      "Loading question dpr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading context dpr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bge large...\n",
      "Loading bm25...\n",
      "Loading splade...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea9c9a75928490e96ebe439275bc303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando corpus: 100%|█████████████████████████████████████| 8674/8674 [00:00<00:00, 2173580.65it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"arguana\"\n",
    "process_corpus(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e5114-ff1a-4dc7-bfb6-cb1812e7bbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
